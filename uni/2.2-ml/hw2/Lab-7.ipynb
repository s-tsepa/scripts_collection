{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab-7: ANN in Pytorch\n",
    "\n",
    "In this lab, you will practice simple deep learning model in one of the most popular frameworks, Pytorch.\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "1. Theoretical issues\n",
    "2. Implementation of basic concepts from scratch \n",
    "3. Get starting in Pytorch\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Theoretical issues\n",
    "Ordinary fully connected neural nets consist of Dense layers, activations, and an output layer.\n",
    "\n",
    "1. What's the difference between deep learning and normal machine learning?\n",
    "2. How does a neural network with no hidden layers and one output neuron compare to a logistic/linear regression?\n",
    "3. How does a neural network with multiple hidden layers but with linear activation and one output neuron compared to logistic/linear regression?\n",
    "4. Can the perceptron find a non-linear decision boundary?\n",
    "5. In a multi-hidden layers network, what's the need of a non-linear activation function?\n",
    "6. Is random weight assignment better than assigning same weights to the units in the hidden layer?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Theory: Feed Forward Neural Network\n",
    "\n",
    "An artificial neural network consists of: \n",
    "\n",
    "- Input Layer  \n",
    "- Hidden Layer(s)\n",
    "- Output Layer\n",
    "\n",
    "<center>\n",
    "<img src=\"./assets/simple_network.png\" alt=\"drawing\" style=\"width:400px;\"/> \n",
    "</center>\n",
    "\n",
    "The layers consist of units, typically called \"neurons\".  Each neuron (except inputs) connects with the neurons from the previous layer. Each connection has a weight. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<center>\n",
    "<img src=\"./assets/simple_neuron.png\" alt=\"drawing\" style=\"width:400px;\"/> \n",
    "</center>\n",
    "\n",
    "Mathematically this equivalent to:\n",
    "<center>\n",
    "\n",
    "$ \n",
    " y = f(w_1x_1 + w_2x_2 + b)\n",
    "$\n",
    " \n",
    "$ \n",
    " y = f\\left(\\sum_{i=1}^{N} w_ix_i + b\\right)\n",
    "$\n",
    "    \n",
    "$\n",
    " y = f(\\vec{x}W + b)\n",
    "$\n",
    "    \n",
    "where $w_{i}$  - weight (just a float number) of connection between i-th neuron from previous layer and the current one; b - bias, one for all connections to this neuron; $x_i$ - the value of i-th neuron. \n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1) Tensors - basic data type in Pytorch\n",
    "\n",
    "It turns out neural network computations are just a sequence of linear algebra operations on tensors, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    " \n",
    "\n",
    "<img src=\"./assets/tensor_examples.svg\" width=\"600px\">\n",
    "\n",
    "Just like Numpy arrays, Pytorch tensors can be added, multiplied, subtracted, etc.\n",
    "\n",
    "#### Let's implement a workflow of simple neuron with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def sigmoid_activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. A reminder: sigmoid function looks like this:\n",
    "\n",
    "<img src='./assets/sigmoid.jpeg' width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate data we need to compute the output of the neuron. We have 5 input features, just random for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data and set the random seed so things are predictable\n",
    "torch.manual_seed(7) \n",
    "\n",
    "# Input features, 5 random normal variables\n",
    "x = torch.randn((1, 5))\n",
    "\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(x)\n",
    "\n",
    "# True bias term\n",
    "bias = torch.randn((1, 1))\n",
    "\n",
    "print(f\"Input vector for neuron: {x}\")\n",
    "print(f\"Weights of input: {weights}\")\n",
    "print(f\"Bias : {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Calculate the output of the neuron with input features `x`, weights `weights`, and bias `bias`. Similar to Numpy, PyTorch has a [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) function, as well as a `.sum()` method on tensors, for taking sums. Feed this linear sum to the `activation` function to complete the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix multiplication in Pytorch\n",
    "In general, matrix multiplication is more preferable than simple summation and multiplication because of  high-performance computing of this operation on modern GPUs. Thus, your next task: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**:  Do the same operation as in **Task 1** using matrix multiplication\n",
    "\n",
    "You may use [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) for multiplication of tensors. \n",
    "\n",
    "Do not forget to reshape one of them \\(`x` or `weights`, that is your task to choose the proper one\\) to avoid an error. You can apply an operator `.view(a,b)` on a tensor to reshape it into $a,b$ shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original shapes, to help you undersand how to reshape\n",
    "# Remember, at the end of computation you sould get a scalar, as in the previous task\n",
    "print(f\"Shape of weight matrix: {weights.shape}\")\n",
    "print(f\"Shape of input vector: {x.shape}\")\n",
    "\n",
    "# Your solution here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Multi layer network\n",
    "\n",
    "We saw how to compute the output of a single unit network. The power of neural networks comes when multiple units are stacked into layers. \n",
    "The output of one layer of neurons becomes the input for the next layer. Now the weights should be expressed as a matrix.\n",
    "\n",
    "<img src='./assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "The bottom layer here are the inputs, surprisingly called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (top) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated: \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output:\n",
    "\n",
    "$$\n",
    "y = f_2(f_1(\\vec{x}\\mathbf{W_2} + B_1)\\mathbf{W_2} + B_2)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) \n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "\n",
    "# Number of input units, must match number of input features\n",
    "n_input = features.shape[1]     \n",
    "# Number of hidden units \n",
    "n_hidden = 2\n",
    "# Number of output units\n",
    "n_output = 1                    \n",
    "\n",
    "# Weights from inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights from hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# Bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Calculate and print the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. The correct value should be `tensor([[0.3171]])` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Pytorch Autograd\n",
    "\n",
    "Training of neural networks through back propagation requires computation of gradients. Pytorch does it for you automatically: tensors track their computational history and support gradient computation if you set the flag `requires_grad=True` in tensor.\n",
    "\n",
    "The `backward()` function is responsible for calculation of gradients and accumulate (not apply) them in respective tensors\n",
    "\n",
    "The tensor with `requires_grad=True` has attribute to check the gradients values : `grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a function of x:  $$f(x) = x^2 + 2x + 1$$\n",
    "\n",
    "The following code will compute and **accumulate** the gradient w.r.t $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute function f(x) = x^2 + 2x + 1\n",
    "z = x ** 2 + 2*x + 1\n",
    "print(z, z.requires_grad)\n",
    "\n",
    "print(f\"Gradient on tensor before backward(): {x.grad}\")\n",
    "# Compute and propagate the gradient\n",
    "z.backward() \n",
    "print(f\"Gradient on tensor after backward(): {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the previous cell several times and see how the value of the gradient changes. Because the gradient is accumulated everytime you call `backward()` it is important to zero the accumulated values before any calculations, i.e., `x.grad = None` or `zero_grad()` for optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent PyTorch from tracking the history and forming the backward graph, the code can be wrapped inside with torch.no_grad(): it will make the code run faster whenever gradient tracking is not needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.tensor(5.0, requires_grad=True)\n",
    "    print(f\"x.requires_grad {x.requires_grad}\")\n",
    "    \n",
    "    z_no_grad = x ** 2 + 2*x + 1\n",
    "    \n",
    "    # Even if x requires gradient, we cannot compute the gradient of function z_no_grad inside this block\n",
    "    print(f\"Value of z: {z_no_grad}, Requires grad?: {z_no_grad.requires_grad}\")\n",
    "    # z.backward()  will trigger an error, because no gradient is tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Tensor to numpy array and vice-versa\n",
    "\n",
    "PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## 2) Model Design in Pytorch\n",
    "Now we're going to build a larger network that can solve a (formerly) difficult problem -  identifying text on an image. We'll use the **MNIST dataset** consisting of grayscale handwritten digits. Each image has 28x28 pixels, you can see the samples below:\n",
    " \n",
    "<img src=\"./assets/mnist.png\" width=\"500px\"> \n",
    "\n",
    "Our goal is to build a neural network that takes one of these images and predicts the corresponding digit. \n",
    "\n",
    "We have three parts that we need to build:\n",
    "1. **Data Loading process** - how to handle data in Pytorch.\n",
    "2. **Model building** - how to create a neural network with desirable parameters.\n",
    "3. **Training loop** - how to properly train the model and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Data Loading\n",
    "\n",
    "1. **Data Transformations** - similar to data preprocessing techniques that we used before, but more image-oriented. You can specify a sequence of transformations, such as normalization, cropping (getting the region of image), resizing, etc. \n",
    "2. **Data Source** - load of built-in (or custom) data sets. For a data set you can specify the transformations from the previous step, and Pytorch will transform each sample automatically.\n",
    "3. **Data Loader** - Pytorch class that makes working with the data sets easy and fast. For example, it can iterate over the batches of data during the training for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We work with a group of samples, i.e. batches, instead of single images.\n",
    "# Usually batch_size is some power of 2.\n",
    "# The bigger batch_size accelerates the training, but requires more memory\n",
    "batch_size = 32\n",
    "test_batch_size = 100\n",
    "\n",
    "# 1. Data Transformations. \n",
    "# transforms.Compose allows to make a series of transformations\n",
    "data_transformations = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           # Normalize an image with mean 0.1307 and standard deviation 0.3081.\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "# 2. Data Source.\n",
    "# Download MNIST dataset, if it's needed, to '../data' folder\n",
    "mnist_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                           transform=data_transformations)\n",
    "# train=False - that's how test and train sets are separated in torchvision.datasets\n",
    "mnist_test = datasets.MNIST('../data', train=False,\n",
    "                            transform=data_transformations)\n",
    "\n",
    "# 3. Data Loader.\n",
    "train_loader = DataLoader(mnist_train,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test,\n",
    "                         batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Get the next batch from loader\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Get the first image from batch. Note, because of shuffling you may get another image\n",
    "print(f\"Label={labels[0]}\")\n",
    "plt.imshow(images[0].reshape(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2) Model building\n",
    "\n",
    "Pytorch provides a module `nn` that makes building networks relatively simple. To create your own network you need to:\n",
    "\n",
    "1. Create a class and inherit it from `nn.Module`. This is the main parent class for all Pytorch models.\n",
    "2. Implement a constructor, i.e. a function `def __init__(self): ...`, where you should describe the architecture of a network and its parameters.\n",
    "3. Define a function `def forward(self, x): ...`, which gets the input batch `x` as an argument, creates  the flow of data through the defined layers and returns the answer.\n",
    "\n",
    "#### Example of the *same network* that we've already implemented from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For layers and models\n",
    "import torch.nn as nn\n",
    "# For activation functions (F.sigmoid is deprecated)\n",
    "import torch.nn.functional as F\n",
    "from torch import sigmoid\n",
    "\n",
    "# Specify the hardware for model run, choose GPU if possible\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# Definition of the very simple network with 1 hidden layer\n",
    "class ToyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Invoke the parent's constructor\n",
    "        super(ToyNet, self).__init__()\n",
    "        # nn implements feedworward layer as nn.Linear(a, b),\n",
    "        # where a, b - input and output dims of weight matrix.\n",
    "        # Bias is included by default.\n",
    "        self.hidden = nn.Linear(3, 2)        \n",
    "        self.output = nn.Linear(2, 1) \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # You can write the result of sigmoid to a local variable,\n",
    "        # but the common style in torch is to re-write x after computation of each layer\n",
    "        x = sigmoid(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return sigmoid(x)\n",
    "\n",
    "\n",
    "model = ToyNet().to(device)\n",
    "\n",
    "print(f\"ToyNet model architecture:\\n {model}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "toy_x = torch.randn((1,3))\n",
    "# Feed the sample to the model like this\n",
    "print(f\"Prediction of {toy_x} : {model(toy_x)}\")\n",
    "# ... or like this\n",
    "print(f\"Prediction of {toy_x} by .forward : {model.forward(toy_x)}\")\n",
    "# You can also feed the batch \n",
    "toy_xs = torch.randn((4,3))\n",
    "print(f\"Prediction on batch: {model(toy_xs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. This network is not trained yet and does some meaningless calculations. Here we just got acquainted with the representation on NN in PyTorch. Training process will be discussed in the further snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions\n",
    "The choice of activation function is important for architecture construction, because it leads to different non-linearity of a layer. For now we worked with sigmoid only, but usually (not necessary) it is applied on the output layer only. **ReLU** (Rectified Linear Unit) is a quite popular activation function for hidden layers: \n",
    "\n",
    "<img src='./assets/ReLU.png' width=300px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Complete the implementation of the following network to solve the classification task on MNIST\n",
    "\n",
    "<img src='./assets/Model.png' width=700px>\n",
    "\n",
    "So, you should specify 2 hidden layers with 256 and 100 neurons respectively, and an output layer with 10 neurons (probability of the classes). Your network will get an image, or 28x28 matrix, and flatten it to 1D array. You also should write the flow of this input array through the network, resulting in a 1x10 array (for one image). Hidden layers should be activited by *ReLU*, output - by *Logarithmyc SoftMax*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Shape of 1st hidden linear layer: (input_array_size, 256)\n",
    "        self.hidden1 = None    \n",
    "        # (256, 100)\n",
    "        self.hidden2 =  None\n",
    "        # (100, 10)\n",
    "        self.output = None \n",
    "\n",
    "    # The batch x is of size (batch, 28 * 28)\n",
    "    def forward(self, x):\n",
    "        # Flatten of 2D image to 1D array\n",
    "        x = x.view(-1, 28*28)\n",
    "        # Complete the flow in hidden layers and output\n",
    "        x = ...\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3) Training loop\n",
    "We should define the loops over the batches and run the training on. Lets specify the hyperparameters of the model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of iterations over the whole data set\n",
    "epochs = 5\n",
    "# Learning rate for Stochastic Gradient Descent\n",
    "lr = 0.01\n",
    "# SGD parameter to accelerate the optimization, check https://paperswithcode.com/method/sgd-with-momentum\n",
    "momentum = 0.5\n",
    "# Loss function - cross entropy, the multiclass variant of that you used in Lab 3\n",
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    # Do not forget to set train() to update weights of model\n",
    "    model.train()\n",
    "    # A wrapper over data loader to show progress bar\n",
    "    bar = tqdm(train_loader)\n",
    "    iteration = 0\n",
    "    overall_loss = 0\n",
    "    for data, target in bar:\n",
    "        # Device of data and model must be the same\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # To avoid an accumulation of gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Prediction \n",
    "        output = model(data)\n",
    "        # Error between prediction and ground truth\n",
    "        loss = criterion(output, target)\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "        # Update params of model\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        overall_loss += loss.item()\n",
    "        bar.set_postfix({\"Loss\": format(overall_loss/iteration, '.6f')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    # Do not forget to set eval() to keep model's params the same \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # Sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f\"Test set: Average loss: {test_loss}, Accuracy: {100. * correct / len(test_loader.dataset)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Collection of optimizers \n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "# Optionally, you can save the params of model to file \n",
    "#torch.save(model.state_dict(), \"mnist_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to check if the neural network model is overfitting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
